{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ee4499",
   "metadata": {},
   "source": [
    "# üó£Ô∏è This notebook will explain the details about RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b218f",
   "metadata": {},
   "source": [
    "### SECTION 1: Creationg embedding using open source model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4804b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import glob\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4856b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"  # This has 384 dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89076f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4778f5",
   "metadata": {},
   "source": [
    "#### Step 1 : Read all the files from the knowledge base folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4294e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knowledge_base_path = \"knowledge-base/**/*.md\"\n",
    "files = glob.glob(pathname=knowledge_base_path, recursive=True)\n",
    "print(f\"No of files: {len(files)}\")\n",
    "\n",
    "entire_knowledge_doc = \"\"\n",
    "for file_path in files:\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        entire_knowledge_doc += f.read()\n",
    "        entire_knowledge_doc += \"\\n\\n\"\n",
    "\n",
    "print(f\"Total Character no is:{len(entire_knowledge_doc):,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad31a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the token count\n",
    "\n",
    "enconding = tiktoken.encoding_for_model(model_name=\"gpt-5-nano\")\n",
    "tokens = enconding.encode(entire_knowledge_doc)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0394b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1ccd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This\"\n",
    "\n",
    "# Embed a single query\n",
    "query_result = embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1038132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea60085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document using Langchain\n",
    "\n",
    "from typing import Text\n",
    "\n",
    "\n",
    "folders = glob.glob(\"knowledge-base/*\")\n",
    "documents = []\n",
    "\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\" ,loader_cls=TextLoader, loader_kwargs={'encoding':'utf-8'})\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)\n",
    "\n",
    "print(len(documents) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ead37d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents=documents)\n",
    "print(len(chunks))\n",
    "\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac60f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\" \n",
    "db_name = \"vector_db\"\n",
    "embedding = HuggingFaceEmbeddings(model=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "vector_store = Chroma.from_documents(documents=chunks, embedding=embedding, persist_directory=db_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e32b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details about the vectors.\n",
    "\n",
    "collections = vector_store._collection\n",
    "count = collections.count()\n",
    "\n",
    "sample_embedding = collections.get(limit = 1 , include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "\n",
    "print(f\"There are {count:,} vectors with {len(sample_embedding):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bea0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd117c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prework\n",
    "\n",
    "result = collections.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "metadatas = result['metadatas']\n",
    "doc_types = [metadata['doc_type'] for metadata in metadatas]\n",
    "colors = [['blue', 'green', 'red', 'orange'][['products', 'employees', 'contracts', 'company'].index(t)] for t in doc_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81365f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edac8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We humans find it easier to visalize things in 2D!\n",
    "# Reduce the dimensionality of the vectors to 2D using t-SNE\n",
    "# (t-distributed stochastic neighbor embedding)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(title='2D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c7b1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 3D!\n",
    "\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    z=reduced_vectors[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=10, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b38ede",
   "metadata": {},
   "source": [
    "## üåê 1st Langchain `RAG` Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8df7013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\PRANAB\\CODE\\OpenAI\\LLMEngineering\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, convert_to_messages\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, ChatHuggingFace\n",
    "import gradio as gr\n",
    "\n",
    "load_dotenv(override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60a205c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the variables\n",
    "\n",
    "CHAT_MODEL_NAME = \"gpt-5-nano\"\n",
    "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\" \n",
    "DB_NAME = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa7c71f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "vector_store = Chroma(persist_directory=DB_NAME, embedding_function=embeddings_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5739f981",
   "metadata": {},
   "source": [
    "#### üå°Ô∏èSome Tricks and Tips on Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abe1fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()\n",
    "llm = ChatOpenAI(temperature=0, model_name=CHAT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8132f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_TEMPLATE = \"\"\"\n",
    "### Role and Persona\n",
    "You are the witty, knowledgeable, and slightly charming official assistant for **Insurellm**. \n",
    "Your goal is to provide accurate information about Insurellm's services.\n",
    "\n",
    "### Instructions\n",
    "1. **Grounding:** Use the provided context to answer the user's questions. Only discuss Insurellm based on this information.\n",
    "2. **Honesty:** If the context does not contain the answer, state clearly that you don't know. Do not provide unnecessary information\n",
    "3. **Tone:** Be helpful, professional, and funny. Avoid being overly formal or robotic.\n",
    "4. **Constraints:** Do not mention \"the context\" or \"the provided documents\" to the user. Speak as if you naturally have this knowledge. Do not provide \n",
    "any extra info that user does not required.\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8cc7f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_question(question: str, history: list[dict]=[]) -> str:\n",
    "    \"\"\"\n",
    "    This method all the question and combine it into a single one to pull the \n",
    "    correct comtext data during the conversation. \n",
    "    Combine only the user message.\n",
    "    \"\"\" \n",
    "    print(history)\n",
    "    prior_question = \"\\n\".join(m[\"content\"][0]['text']  for m in history if m[\"role\"] == \"user\")\n",
    "    return prior_question + \"\\n\" + question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63217fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question:str, history):\n",
    "\n",
    "    combined_question = combine_question(question=question, history=history)\n",
    "    docs = retriever.invoke(combined_question, k=3)\n",
    "\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    system_prompt = SYSTEM_PROMPT_TEMPLATE.format(context=context)\n",
    "    messages = [SystemMessage(content=system_prompt)]\n",
    "    messages.extend(convert_to_messages(history)) #Append the dict list to another list of dict\n",
    "    messages.append(HumanMessage(content=question)) #Append the dict to another list of dict\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a067936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat).launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
